{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Numerai Starter Kit",
      "provenance": [],
      "authorship_tag": "ABX9TyPp8qIF+H76FpZtxntdgw/9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djliden/numerai_starter_kit/blob/main/Numerai_Starter_Kit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGlc6WXctP3Y"
      },
      "source": [
        "# 1 Introduction\n",
        "This notebook will walk you through the entire process of making a [numerai](numer.ai) submission, from downloading the data to submitting final predictions, all in a google colab notebook. In particular, it will address two challenges:\n",
        "- handling API keys in a remote environment (colab)\n",
        "- parsing the large CSV files which, if read all at once, will exceed colab's memory and cause the notebook to crash.\n",
        "\n",
        "This notebook will implement two models: a basic tabular neural network using `fastai` and a linear regression model using `scikit-learn`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma62H4nY5DOt"
      },
      "source": [
        "## 1.1 Installing and Importing Dependencies\n",
        "First, we install and import the necessary packages. This cell is currently set *not* to print any output; if you run into any issues and need to check for error messages, comment out the `%%capture` line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fitBsr_ty8_"
      },
      "source": [
        "%%capture\n",
        "# install\n",
        "!pip install --upgrade python-dotenv fastai numerapi\n",
        "\n",
        "# import dependencies\n",
        "import gc\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from getpass import getpass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numerapi\n",
        "from fastai.tabular.all import *\n",
        "from pathlib import Path\n",
        "from scipy.stats import spearmanr\n",
        "import sklearn.linear_model\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8k1mucsueRZ"
      },
      "source": [
        "## 1.2 Setting Up numerapi\n",
        "We will use the [numerapi](https://github.com/uuazed/numerapi) package to access the data and make submissions. For this to work, numerapi needs to use your API keys (which can be obtained [here](https://numer.ai/submit)). We will set up two main ways of passing these API keys to a numerapi instance:\n",
        "1. Read a `.env` file using the `python-dotenv` package. This will require you to upload a `.env` file (which contains your secret key and should *not* be kept under version control). Using this method means you will not have to directly enter your keys each time you use this notebook, though you will need to re-upload the `.env` file.\n",
        "2. Manually entering the API keys -- if you don't have access to, or don't want to mess with, your `.env` file.\n",
        "\n",
        "If you have a `.env` file, upload it to the default working directory, `content`, now. In either case, run the cell below to set up the numerapi instance. See [Appendix A](#app_a) for instructions on generating and downloading a .env file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z1CS2Uwv79C"
      },
      "source": [
        "# Load the numerapi credentials from .env or prompt for them if not available\n",
        "def credential():\n",
        "    dotenv_path = find_dotenv()\n",
        "    load_dotenv(dotenv_path)\n",
        "\n",
        "    if os.getenv(\"NUMERAI_PUBLIC_KEY\"):\n",
        "        print(\"Loaded Numerai Public Key into Global Environment!\")\n",
        "    else:\n",
        "        os.environ[\"NUMERAI_PUBLIC_KEY\"] = getpass(\"Please enter your Numerai Public Key. You can find your key here: https://numer.ai/submit -> \")\n",
        "    \n",
        "    if os.getenv(\"NUMERAI_SECRET_KEY\"):\n",
        "        print(\"Loaded Numerai Secret Key into Global Environment!\")\n",
        "    else:\n",
        "        os.environ[\"NUMERAI_SECRET_KEY\"] = getpass(\"Please enter your Numerai Secret Key. You can find your key here: https://numer.ai/submit -> \")\n",
        "    \n",
        "    if os.getenv(\"NUMERAI_MODEL_ID\"):\n",
        "        print(\"Loaded Numerai Model ID into Global Environment!\")\n",
        "    else:\n",
        "        os.environ[\"NUMERAI_MODEL_ID\"] = getpass(\"Please enter your Numerai Model ID. You can find your key here: https://numer.ai/submit -> \")\n",
        "\n",
        "credential()\n",
        "public_key = os.environ.get(\"NUMERAI_PUBLIC_KEY\")\n",
        "secret_key = os.environ.get(\"NUMERAI_SECRET_KEY\")\n",
        "model_id = os.environ.get(\"NUMERAI_MODEL_ID\")\n",
        "napi = numerapi.NumerAPI(verbosity=\"info\", public_id=public_key, secret_key=secret_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1LhPvUS7RLk"
      },
      "source": [
        "You can read up on the functionality of numerapi [here](https://github.com/uuazed/numerapi). You can use it to download the competition data, view other numerai users' public profiles, check submission status, manage your stake, and much more. In this case, we'll only be using it to download competition data and submit predictions.\n",
        "\n",
        "## 1.3 Downloading Competition Data\n",
        "In a more structured project, you'll probably want to keep the data in a seprate directory from your scripts etc. You could also link google colab to your google drive and store the data there in order to avoid needing to download and process the data every time. In this case, however, we'll keep everything in `./content`, and download the data fresh each time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcCNcVMv7y1B"
      },
      "source": [
        "napi.download_current_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm7lo6fe8Qsy"
      },
      "source": [
        "## 1.4 Generating the Training Sample\n",
        "\n",
        "If you look at the files we downloaded above, you'll see a `numerai_tournament_data.csv` file and a `numerai_training_data.csv` file. The \"tournament\" file contains many rows with targets which we can use for validation, so let's extract those and combine them with our training set. Note that this cell saves a new `csv` after combining the training and validation data, so we can avoid the time-consuming parsing process if we run this cell again in the same session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd06RGi4840k"
      },
      "source": [
        "tourn_file = Path(f'./numerai_dataset_{napi.get_current_round()}/numerai_tournament_data.csv')\n",
        "train_file = Path(f'./numerai_dataset_{napi.get_current_round()}/numerai_training_data.csv')\n",
        "processed_train_file = Path('./training_processed.csv')\n",
        "\n",
        "if processed_train_file.exists():\n",
        "    print(\"Loading the processed training data from file\\n\")\n",
        "    training_data = pd.read_csv(processed_train_file)\n",
        "else:\n",
        "    tourn_iter_csv = pd.read_csv(tourn_file, iterator=True, chunksize=1e6)\n",
        "    val_df = pd.concat([chunk[chunk['data_type'] == 'validation'] for chunk in tqdm(tourn_iter_csv)])\n",
        "    tourn_iter_csv.close()\n",
        "    training_data = pd.read_csv(train_file)\n",
        "    training_data = pd.concat([training_data, val_df])\n",
        "    training_data.reset_index(drop=True, inplace=True)\n",
        "    print(\"Training Dataset Generated! Saving to file ...\")\n",
        "    training_data.to_csv(processed_train_file, index=False)\n",
        "\n",
        "\n",
        "feature_cols = training_data.columns[training_data.columns.str.startswith('feature')]\n",
        "target_cols = ['target']\n",
        "\n",
        "train_idx = training_data.index[training_data.data_type=='train'].tolist()\n",
        "val_idx = training_data.index[training_data.data_type=='validation'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BSr1BMR5Ilh"
      },
      "source": [
        "# 2 Modeling the Data\n",
        "\n",
        "In this section, we will define our evaluation metrics; run two different models (a linear regression model from `scikit-learn` and a neural network from `fastai`); and generate submission dataframes from those files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeYfQcry6svF"
      },
      "source": [
        "## 2.1 Evaluation Metrics\n",
        "\n",
        "In this section, we will define two key evaluation metrics used to assess the performance of models before submitting to the tournament. These metrics are:\n",
        "- Average Spearman Correlation per era: The sum of each era's Spearman correlation divided by the number of eras.\n",
        "- Sharpe Ratio: The average correlation per era divided by the standard deviation of the correlations per era.\n",
        "\n",
        "Both are defined in reasonable detail [here](https://wandb.ai/carlolepelaars/numerai_tutorial/reports/How-to-get-Started-With-Numerai--VmlldzoxODU0NTQ). The methods defined below are modified versions of the methods described in that post."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtFniCX98z6i"
      },
      "source": [
        "def corr(df: pd.DataFrame) -> np.float32:\n",
        "    \"\"\"\n",
        "    Calculate the correlation by using grouped per-era data\n",
        "    :param df: A Pandas DataFrame containing the columns \"era\", \"target\" and \"prediction\"\n",
        "    :return: The average per-era correlations.\n",
        "    \"\"\"\n",
        "    def _score(sub_df: pd.DataFrame) -> np.float32:\n",
        "        \"\"\" Calculate Spearman correlation for Pandas' apply method \"\"\"\n",
        "        return spearmanr(sub_df[\"target\"],  sub_df[\"prediction\"])[0]\n",
        "    corrs = df.groupby(\"era\").apply(_score)\n",
        "    return corrs.mean() \n",
        "\n",
        "def sharpe(df: pd.DataFrame) -> np.float32:\n",
        "    \"\"\"\n",
        "    Calculate the Sharpe ratio by using grouped per-era data\n",
        "    :param df: A Pandas DataFrame containing the columns \"era\", \"target\" and \"prediction\"\n",
        "    :return: The Sharpe ratio for your predictions.\n",
        "    \"\"\"\n",
        "    def _score(sub_df: pd.DataFrame) -> np.float32:\n",
        "        \"\"\" Calculate Spearman correlation for Pandas' apply method \"\"\"\n",
        "        return spearmanr(sub_df[\"target\"],  sub_df[\"prediction\"])[0]\n",
        "    corrs = df.groupby(\"era\").apply(_score)\n",
        "    return corrs.mean() / corrs.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4azmVvR6ZhT"
      },
      "source": [
        "## 2.2 Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3qwFNYGF5TB"
      },
      "source": [
        "### 2.2.1 Linear Regression Model\n",
        "This model closely follows the tutorial example [here](https://colab.research.google.com/github/numerai/example-scripts/blob/master/making-your-first-submission-on-numerai.ipynb). We will use the `scikit-learn` package, with which we can implement and fit our regression model in just a couple of lines of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-el1HtBefnb"
      },
      "source": [
        "#### Fitting the Linear Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKG75I7-yKvA"
      },
      "source": [
        "train_sample = training_data.iloc[train_idx]\n",
        "model = sklearn.linear_model.LinearRegression()\n",
        "model.fit(train_sample[feature_cols], train_sample.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inUbdhvb66Pb"
      },
      "source": [
        "#### Assessing Regression Model Performance\n",
        "\n",
        "Here we apply the `corr` and `sharpe` methods defined above to predictions made on the validation sample in order to estimate our model's tournament performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgU0Pz9x9vpv"
      },
      "source": [
        "val_sample = training_data.iloc[val_idx]\n",
        "val_preds = model.predict(val_sample[feature_cols])\n",
        "eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                        'target':val_sample.target,\n",
        "                        'era':val_sample.era}).reset_index()\n",
        "val_sharpe = sharpe(eval_df)\n",
        "val_corr = corr(eval_df)\n",
        "\n",
        "print((f'The linear regression model\\'s validation correlation is {val_corr}. '\n",
        "       f'Its validation sharpe is {val_sharpe}'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRLGopZF0r7T"
      },
      "source": [
        "#### Making Predictions with the Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOeJjJdZ08aV"
      },
      "source": [
        "ids = []\n",
        "preds = []\n",
        "\n",
        "chunksize = 50000\n",
        "\n",
        "tourn_iter_csv = pd.read_csv(tourn_file, iterator=True, chunksize=1e6)\n",
        "for chunk in tourn_iter_csv:\n",
        "    df = chunk[feature_cols]\n",
        "    out = model.predict(df)\n",
        "    ids.extend(chunk[\"id\"])\n",
        "    preds.extend(out)\n",
        "tourn_iter_csv.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9HN8lrw2zd_"
      },
      "source": [
        "linear_regression_predictions_df = pd.DataFrame({\n",
        "    'id':ids,\n",
        "    'prediction':preds\n",
        "})\n",
        "linear_regression_predictions_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg_HdPki6Mh3"
      },
      "source": [
        "linear_regression_predictions_df.to_csv(\"lr_predictions.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEwbRegtWrRE"
      },
      "source": [
        "#### Submitting Predictions from the Linear Regression Model\n",
        "\n",
        "We can use `numerapi` to submit these predictions as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW-zccdgW0GX"
      },
      "source": [
        "napi.upload_predictions(\"lr_predictions.csv\", model_id=os.environ.get(\"NUMERAI_MODEL_ID\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVNIGfpzbnF_"
      },
      "source": [
        "### 2.2.2 Fastai Neural Network\n",
        "In this section, we use fastai's tabular data processing and modeling capabilities to develop a neural network ([fastai](https://docs.fast.ai/tabular.core.html))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iikrDZyocTlq"
      },
      "source": [
        "#### Preparing the Data\n",
        "\n",
        "We use the [`TabularPandas`](https://docs.fast.ai/tabular.core.html#TabularPandas) class to set up our DataLoaders. We will use a relatively large batch size (`bs=2048`) to facilitate rapid training in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhykYYmGb-EP"
      },
      "source": [
        "splits = (list(train_idx), list(val_idx))\n",
        "data = TabularPandas(training_data, cat_names=None,\n",
        "                    cont_names=list(feature_cols.values),\n",
        "                    y_names=target_cols, splits = splits)\n",
        "\n",
        "dls = data.dataloaders(bs = 2048)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "navE8N37c6PP"
      },
      "source": [
        "#### Set Up and Fit Fastai Model\n",
        "\n",
        "In this simple example, we will make no attempt to optimize hyperparameters. You can experiment with the batch size, which we set above, along with the number and size of layers, weight decay, and many of the additional configuration options listed [here](https://docs.fast.ai/tabular.model.html#TabularModel).\n",
        "\n",
        "**Note:** The following cell will run much faster if you are using a colab session with GPU enabled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is7r4_Itc9Jt"
      },
      "source": [
        "learn = tabular_learner(dls, layers=[200,200],\n",
        "                        loss_func=MSELossFlat(),\n",
        "                        metrics = [PearsonCorrCoef()])\n",
        "\n",
        "learn.fit_one_cycle(3, wd = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCPQihkDekvO"
      },
      "source": [
        "#### Evaluate Performance of Fastai Model\n",
        "\n",
        "As we did with the regression model, we'll check the validation correlation and sharpe score of this fastai neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm5p1ddieQxC"
      },
      "source": [
        "prediction, target = learn.get_preds()\n",
        "prediction = prediction.numpy().squeeze()\n",
        "target = target.numpy().squeeze()\n",
        "\n",
        "era = dls.valid_ds.items['era']\n",
        "eval_df = pd.DataFrame({'prediction':prediction, 'target':target, 'era':era}).reset_index()\n",
        "\n",
        "val_sharpe = sharpe(eval_df)\n",
        "val_corr = corr(eval_df)\n",
        "\n",
        "print((f'The fastai tabular neural network\\'s validation correlation is {val_corr}. '\n",
        "       f'Its validation sharpe is {val_sharpe}'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Firq8-TFf2UV"
      },
      "source": [
        "#### Making Predictions with the Fastai Model\n",
        "\n",
        "There is probably a way to significantly simplify the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hrO_RNsf5S3"
      },
      "source": [
        "ids = []\n",
        "preds = []\n",
        "\n",
        "chunksize = 75000\n",
        "\n",
        "tourn_iter_csv = pd.read_csv(tourn_file, iterator=True, chunksize=chunksize)\n",
        "for chunk in tourn_iter_csv:\n",
        "    chunk.drop(columns = 'target', inplace = True)\n",
        "    test_dl = dls.test_dl(chunk)\n",
        "    preds_out,_ = learn.get_preds(dl = test_dl, inner = True)\n",
        "    preds_out = preds_out.tolist()\n",
        "    preds_out = [item for sublist in preds_out for item in sublist]\n",
        "    ids_out = chunk[\"id\"]\n",
        "    preds.extend(preds_out)\n",
        "    ids.extend(ids_out)\n",
        "tourn_iter_csv.close()\n",
        "#preds_out = [item for sublist in preds for item in sublist]\n",
        "#ids_out = [item for sublist in ids for item in sublist]\n",
        "\n",
        "fastai_predictions_df = pd.DataFrame({\n",
        "'id':ids,\n",
        "'prediction':preds\n",
        "})\n",
        "\n",
        "fastai_predictions_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6TCiPUxmgS4"
      },
      "source": [
        "#### Submitting Predictions from the Fastai Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjmfwFJ5mjhp"
      },
      "source": [
        "fastai_predictions_df.to_csv(\"fastai_predictions.csv\", index=False)\n",
        "napi.upload_predictions(\"fastai_predictions.csv\", model_id=os.environ.get(\"NUMERAI_MODEL_ID\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PifrZTKkENq2"
      },
      "source": [
        "<a name=\"app_a\"></a>\n",
        "# Appendix A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQacl3Qux3Xx"
      },
      "source": [
        "## Generating and Saving a `.env` file\n",
        "I recommend filling out this section to generate a `.env` file and then downloading that file for future use. Then, the next time you want to run this notebook, upload the `.env` file and you will not need to enter your credentials manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1Ytb6Fqx3EQ"
      },
      "source": [
        "# Write lines to file\n",
        "\n",
        "with open('./.env', 'w') as dotenv:\n",
        "    dotenv.write(f'NUMERAI_PUBLIC_KEY = {getpass(\"Please enter your Numerai Public Key. You can find your key here: https://numer.ai/submit -> \")}\\n')\n",
        "    dotenv.write(f'NUMERAI_SECRET_KEY = {getpass(\"Please enter your Numerai Secret Key. You can find your key here: https://numer.ai/submit -> \")}\\n')\n",
        "    dotenv.write(f'NUMERAI_MODEL_ID = {getpass(\"Please enter your Numerai Model ID. You can find your key here: https://numer.ai/submit -> \")}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhCmGbzd2a3A"
      },
      "source": [
        "To confirm that this worked, you can run `!cat .env` in a new cell and check the output against the values on https://numer.ai/submit. Make sure no one is looking, and that you don't save the output where others on the Internet can find it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7NR2bcX3Lv2"
      },
      "source": [
        "If you want to download this `.env` file for future use (e.g. to re-upload the next time you want to use this notebook), run the next cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agIJASIm0rOe"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"./.env\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}